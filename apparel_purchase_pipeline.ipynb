{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1681b29b-fa5c-49ca-8b3a-9e46ce2ec3f9",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "from collections import Counter\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, roc_curve\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa23d541-9193-4cc4-92a4-2a95cdaa29cb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def safe_read_csv(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    return pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0742c60-af6a-4f50-a07e-32dfe057452a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def parse_categories_raw(cat_value):\n",
        "    if pd.isna(cat_value):\n",
        "        return []\n",
        "    if isinstance(cat_value, (list, tuple, set)):\n",
        "        return [str(x).strip() for x in cat_value if str(x).strip()!='']\n",
        "    s = str(cat_value).strip()\n",
        "    if s.startswith('[') and s.endswith(']'):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(s)\n",
        "            if isinstance(parsed, (list, tuple, set)):\n",
        "                return [str(x).strip() for x in parsed if str(x).strip()!='']\n",
        "        except Exception:\n",
        "            pass\n",
        "    for sep in ['|', ',', ';']:\n",
        "        if sep in s:\n",
        "            parts = [p.strip() for p in s.split(sep) if p.strip()!='']\n",
        "            return parts\n",
        "    return [s] if s!='' else []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c55b246f-f502-43f9-8cb5-f330ba65280c",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def build_purchase_aggregates(purchases_df, snapshot_date=None):\n",
        "    df = purchases_df.copy()\n",
        "    for col in ['date', 'price', 'quantity', 'category_ids', 'client_id']:\n",
        "        if col not in df.columns:\n",
        "            raise KeyError(f\"Purchases missing expected column: {col}\")\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0.0)\n",
        "    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0.0)\n",
        "    df['amount'] = df['price'] * df['quantity']\n",
        "\n",
        "    if snapshot_date is None:\n",
        "        snapshot_date = df['date'].max() + pd.Timedelta(days=1)\n",
        "    else:\n",
        "        snapshot_date = pd.to_datetime(snapshot_date)\n",
        "\n",
        "    agg = df.groupby('client_id').agg(\n",
        "        total_amount=('amount', 'sum'),\n",
        "        total_quantity=('quantity', 'sum'),\n",
        "        avg_price=('price', 'mean'),\n",
        "        first_purchase=('date', 'min'),\n",
        "        last_purchase=('date', 'max'),\n",
        "        n_transactions=('date', 'nunique')\n",
        "    ).reset_index()\n",
        "\n",
        "    agg['recency_days'] = (snapshot_date - agg['last_purchase']).dt.days\n",
        "    agg['customer_age_days'] = (snapshot_date - agg['first_purchase']).dt.days\n",
        "    agg['avg_amount_per_tx'] = agg['total_amount'] / agg['n_transactions'].replace(0, np.nan)\n",
        "\n",
        "    df['category_tokens'] = df['category_ids'].apply(parse_categories_raw)\n",
        "    grouped = df.groupby('client_id')['category_tokens'].agg(list).reset_index()\n",
        "\n",
        "    rows = []\n",
        "    for _, r in grouped.iterrows():\n",
        "        cid = r['client_id']\n",
        "        lists = r['category_tokens']\n",
        "        cnt = Counter()\n",
        "        for l in lists:\n",
        "            cnt.update(l)\n",
        "        unique_count = len(cnt)\n",
        "        if len(cnt) > 0:\n",
        "            top_cat, top_cnt = cnt.most_common(1)[0]\n",
        "            top3 = [c for c,_ in cnt.most_common(3)]\n",
        "            top3_str = '|'.join(top3)\n",
        "        else:\n",
        "            top_cat, top_cnt, top3_str = None, 0, ''\n",
        "        rows.append({\n",
        "            'client_id': cid,\n",
        "            'unique_category_count': unique_count,\n",
        "            'top_category': top_cat,\n",
        "            'top_category_count': top_cnt,\n",
        "            'top_3_categories': top3_str\n",
        "        })\n",
        "    cat_df = pd.DataFrame(rows)\n",
        "\n",
        "    agg = agg.merge(cat_df, on='client_id', how='left')\n",
        "\n",
        "    agg['unique_category_count'] = agg['unique_category_count'].fillna(0).astype(int)\n",
        "    agg['avg_price'] = agg['avg_price'].fillna(0.0)\n",
        "    agg['avg_amount_per_tx'] = agg['avg_amount_per_tx'].fillna(0.0)\n",
        "    agg['recency_days'] = agg['recency_days'].fillna((snapshot_date - df['date'].min()).days + 1)\n",
        "\n",
        "    return agg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac691d1-7dc1-4e9e-b5d7-ea9d3c2cb277",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def assemble_features(purchase_agg, message_agg, target_df):\n",
        "    df = target_df.merge(purchase_agg, on='client_id', how='left')\n",
        "    df = df.merge(message_agg, on='client_id', how='left')\n",
        "\n",
        "    df['avg_amount_per_cat'] = df['avg_amount_per_tx'] / (df['unique_category_count'].replace(0, 1))\n",
        "    df['frequency'] = df['n_transactions'].fillna(0)\n",
        "    df['ever_bought'] = (~df['first_purchase'].isna()).astype(int)\n",
        "\n",
        "    if 'top_category' in df.columns:\n",
        "        df['top_category'] = df['top_category'].fillna('missing').astype(str)\n",
        "    else:\n",
        "        df['top_category'] = 'missing'\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    df[num_cols] = df[num_cols].fillna(0.0)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1260606c-ce44-4cad-8ffc-d95554f9e0ce",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_for_model(X_train, X_test):\n",
        "    num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    num_imp = SimpleImputer(strategy='median')\n",
        "    X_train_num = pd.DataFrame(num_imp.fit_transform(X_train[num_cols]), columns=num_cols, index=X_train.index)\n",
        "    X_test_num = pd.DataFrame(num_imp.transform(X_test[num_cols]), columns=num_cols, index=X_test.index)\n",
        "\n",
        "    X_train_cat = X_train[cat_cols].fillna('missing').astype(str).copy()\n",
        "    X_test_cat = X_test[cat_cols].fillna('missing').astype(str).copy()\n",
        "\n",
        "    X_train_prep = pd.concat([X_train_num, X_train_cat], axis=1)\n",
        "    X_test_prep = pd.concat([X_test_num, X_test_cat], axis=1)\n",
        "\n",
        "    return X_train_prep, X_test_prep, num_cols, cat_cols, num_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1c772a-ba1c-4f6c-aad4-6428a8306ec7",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def find_best_threshold(y_true, y_proba):\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1\n",
        "    for t in np.linspace(0.01, 0.99, 99):\n",
        "        f1 = f1_score(y_true, (y_proba >= t).astype(int))\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    return best_t, best_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4faac544-9491-4422-a5a4-266454cafc25",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(purchases_path, messages_path, target_path, outdir='output', snapshot_date=None):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    readme = \"\"\"# Проект: предсказание покупки в течение 90 дней\n",
        "\n",
        "Инструкция:\n",
        "1. Подготовьте CSV-файлы apparel-purchases.csv, apparel-messages.csv, apparel-target_binary.csv\n",
        "2. Запустите: `python apparel_purchase_pipeline.py --purchases apparel-purchases.csv --messages apparel-messages.csv --target apparel-target_binary.csv`\n",
        "3. Смотрите результаты в папке output: model.joblib, feature_importances.csv, metrics.txt, roc_curve.png\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    req = \"\"\"\n",
        "pandas\n",
        "numpy\n",
        "scikit-learn\n",
        "joblib\n",
        "matplotlib\n",
        "\"\"\"\n",
        "    with open(os.path.join(outdir, 'README.md'), 'w', encoding='utf-8') as f:\n",
        "        f.write(readme)\n",
        "    with open(os.path.join(outdir, 'requirements.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(req)\n",
        "\n",
        "    purchases = safe_read_csv(purchases_path)\n",
        "    messages = safe_read_csv(messages_path)\n",
        "    target = safe_read_csv(target_path)\n",
        "\n",
        "    print(\"Building purchase aggregates...\")\n",
        "    purchase_agg = build_purchase_aggregates(purchases, snapshot_date=snapshot_date)\n",
        "\n",
        "    print(\"Building message aggregates...\")\n",
        "    message_agg = build_message_aggregates(messages)\n",
        "\n",
        "    print(\"Assembling features...\")\n",
        "    df = assemble_features(purchase_agg, message_agg, target)\n",
        "\n",
        "    drop_cols = ['first_purchase', 'last_purchase']\n",
        "    X = df.drop(columns=['client_id', 'target'] + [c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "    y = df['target']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "    X_train_prep, X_test_prep, num_cols, cat_cols, num_imputer = preprocess_for_model(X_train, X_test)\n",
        "\n",
        "    print(\"Num features:\", len(num_cols), \"Cat features:\", len(cat_cols))\n",
        "\n",
        "    pos = y_train.sum()\n",
        "    neg = len(y_train) - pos\n",
        "    if pos == 0:\n",
        "        raise ValueError(\"No positive examples in training set!\")\n",
        "    weight_ratio = max(1.0, int(round(neg / pos)))\n",
        "    class_weights = [1, weight_ratio]\n",
        "    print(\"Using class_weights:\", class_weights)\n",
        "\n",
        "    cbc = CatBoostClassifier(\n",
        "        eval_metric='AUC',\n",
        "        loss_function='Logloss',\n",
        "        random_seed=42,\n",
        "        verbose=0,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    param_grid = {\n",
        "        'depth': [4, 6],\n",
        "        'learning_rate': [0.03, 0.05],\n",
        "        'iterations': [300, 500]\n",
        "    }\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    grid = GridSearchCV(estimator=cbc, param_grid=param_grid, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
        "\n",
        "    cat_indices = [list(X_train_prep.columns).index(c) for c in cat_cols] if len(cat_cols) > 0 else []\n",
        "    grid.fit(X_train_prep.values, y_train.values, **({'cat_features': cat_indices} if len(cat_indices)>0 else {}))\n",
        "\n",
        "    best = grid.best_estimator_\n",
        "    print(\"Best params:\", grid.best_params_)\n",
        "\n",
        "    y_proba = best.predict_proba(X_test_prep.values)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    best_t, best_f1 = find_best_threshold(y_test.values, y_proba)\n",
        "    y_pred = (y_proba >= best_t).astype(int)\n",
        "\n",
        "    print(f\"Test ROC AUC: {auc:.4f}\")\n",
        "    print(\"Best threshold (by F1 on test):\", best_t, \"best_f1:\", best_f1)\n",
        "    print(\"Classification report (threshold chosen):\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    joblib.dump(best, os.path.join(outdir, 'model_catboost.joblib'))\n",
        "    joblib.dump(num_imputer, os.path.join(outdir, 'num_imputer.joblib'))\n",
        "\n",
        "    meta = {\n",
        "        'num_cols': num_cols,\n",
        "        'cat_cols': cat_cols,\n",
        "        'features': X_train_prep.columns.tolist(),\n",
        "        'best_params': grid.best_params_,\n",
        "        'best_threshold': float(best_t)\n",
        "    }\n",
        "    joblib.dump(meta, os.path.join(outdir, 'meta.joblib'))\n",
        "\n",
        "    try:\n",
        "        fi = best.get_feature_importance(prettified=False)\n",
        "        fi_df = pd.DataFrame({'feature': X_train_prep.columns.tolist(), 'importance': fi})\n",
        "        fi_df.sort_values('importance', ascending=False).to_csv(os.path.join(outdir, 'feature_importances.csv'), index=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    with open(os.path.join(outdir, 'metrics.txt'), 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"roc_auc: {auc}\\n\")\n",
        "        f.write(f\"best_threshold: {best_t}\\n\")\n",
        "        f.write(f\"best_f1: {best_f1}\\n\\n\")\n",
        "        f.write(\"classification_report:\\n\")\n",
        "        f.write(classification_report(y_test, y_pred))\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    plt.figure(); plt.plot(fpr, tpr); plt.xlabel('FPR'); plt.ylabel('TPR')\n",
        "    plt.title(f'ROC AUC = {auc:.4f}'); plt.grid(True)\n",
        "    plt.savefig(os.path.join(outdir, 'roc_curve.png')); plt.close()\n",
        "\n",
        "    print(\"Artifacts saved to\", outdir)\n",
        "    return best, meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fd296af-e7ef-4eea-a88b-b0982afc4d38",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    purchases_file = \"apparel-purchases.csv\"\n",
        "    messages_file = \"apparel-messages.csv\"\n",
        "    target_file = \"apparel-target_binary.csv\"\n",
        "    output_dir = \"output\"\n",
        "    snapshot_date = None\n",
        "\n",
        "    train_and_evaluate(purchases_file, messages_file, target_file, outdir=output_dir, snapshot_date=snapshot_date)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
